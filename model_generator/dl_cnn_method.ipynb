{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAS503 : Intro to Data Driven Analysis\n",
    "\n",
    "\n",
    "## Final Project Group 26\n",
    "\n",
    "---\n",
    "SUBMITTED BY -\n",
    "---\n",
    "| Name | Person ID | Email |\n",
    "| --- | --- | --- |\n",
    "| AMAN PRAKASH | 50416755 | amanprak@buffalo.edu |\n",
    "| Prashant Upadhyay | 50419393 | pupadhya@buffalo.edu |\n",
    "| Serath Chandra Nutakki | 50363265 | serathch@buffalo.edu |\n",
    "| Vamshivardhan Reddy Balannagari | 50435533 | vamshiva@buffalo.edu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import librosa\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_songs(X, y, w = 0.05, o = 0.5):\n",
    "    var_x,var_y = [],[]\n",
    "\n",
    "    x = X.shape[0]\n",
    "    chunk = int(x*w)\n",
    "    offset = int(chunk*(1.-o))\n",
    "    \n",
    "    split_song = [X[i:i+chunk] for i in range(0, x - chunk + offset, offset)]\n",
    "    for s in split_song:\n",
    "        if s.shape[0] != chunk:\n",
    "            continue\n",
    "        var_x.append(s)\n",
    "        var_y.append(y)\n",
    "\n",
    "    return np.array(var_x), np.array(var_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melspectrogram_generator(songs, n_fft=1024, hop_length=256):\n",
    "    mel_spec = lambda x: librosa.feature.melspectrogram(x, n_fft=n_fft,\n",
    "        hop_length=hop_length, n_mels=128)[:,:,np.newaxis]\n",
    "\n",
    "    tsongs = map(mel_spec, songs)\n",
    "    return np.array(list(tsongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y):\n",
    "    specs_arr = []\n",
    "    genres_arr = []\n",
    "    \n",
    "    for fn, genre in zip(X, y):\n",
    "        signal, sr = librosa.load(fn)\n",
    "        signal = signal[:song_samples]\n",
    "        signals, y = split_songs(signal, genre)\n",
    "        specs = melspectrogram_generator(signals)\n",
    "        genres_arr.extend(y)\n",
    "        specs_arr.extend(specs)\n",
    "    \n",
    "    return np.array(specs_arr), to_categorical(specs_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(src_dir, genres, song_samples):    \n",
    "    fu_arr, genres_arr = [], []\n",
    "\n",
    "    for i,_ in genres.items():\n",
    "        folder = src_dir + i\n",
    "        for root, subdirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                file_name = folder + \"/\" + file\n",
    "\n",
    "                fu_arr.append(file_name)\n",
    "                genres_arr.append(genres[i])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        fu_arr, genres_arr, test_size=0.25, random_state=42, stratify=genres_arr)\n",
    "    \n",
    "    X_train, y_train = data_split(X_train, y_train)\n",
    "    X_test, y_test = data_split(X_test, y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan_dir = '../data/genres/'\n",
    "song_samples = 660000\n",
    "genres = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4, \n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "X_train, X_test, y_train, y_test = fetch_data(gtzan_dir, genres, song_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, cnt = np.unique(np.argmax(y_train, axis=1), return_counts=True)\n",
    "plt.bar(val, cnt)\n",
    "\n",
    "val, cnt = np.unique(np.argmax(y_test, axis=1), return_counts=True)\n",
    "plt.bar(val, cnt)\n",
    "print(\"Train and Test Set Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class gtzan_generator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=64, is_test = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __augment(self, sig, hflip = 0.5, random_cutout = 0.5):\n",
    "        spectrograms =  []\n",
    "        for s in sig:\n",
    "            signal = copy(s)\n",
    "            if np.random.rand() < hflip:\n",
    "                signal = np.flip(signal, 1)\n",
    "            if np.random.rand() < random_cutout:\n",
    "                lines = np.random.randint(signal.shape[0], size=3)\n",
    "                cols = np.random.randint(signal.shape[0], size=4)\n",
    "                signal[lines, :, :] = -80\n",
    "                signal[:, cols, :] = -80\n",
    "\n",
    "            spectrograms.append(signal)\n",
    "        return np.array(spectrograms)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sig = self.X[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        if not self.is_test:\n",
    "            sig = self.__augment(signals)\n",
    "        return sig, self.y[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(x_value, n_filters, pool_size=(2, 2)):\n",
    "    x_value = Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same')(x_value)\n",
    "    x_value = Activation('relu')(x_value)\n",
    "    x_value = MaxPooling2D(pool_size=pool_size, strides=pool_size)(x_value)\n",
    "    x_value = Dropout(0.25)(x_value)\n",
    "    return x_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_shape, num_genres):\n",
    "    inpt = Input(shape=input_shape)\n",
    "    x_value = convolution_block(inpt, 16)\n",
    "    x_value = convolution_block(x_value, 32)\n",
    "    x_value = convolution_block(x_value, 64)\n",
    "    x_value = convolution_block(x_value, 128)\n",
    "    x_value = convolution_block(x_value, 256)\n",
    "\n",
    "    x_value = Flatten()(x_value)\n",
    "    x_value = Dropout(0.5)(x_value)\n",
    "    x_value = Dense(512, activation='relu', \n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.02))(x_value)\n",
    "    x = Dropout(0.25)(x_value)\n",
    "    predictions = Dense(num_genres, \n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.02))(x_value)\n",
    "    \n",
    "    model = Model(inputs=inpt, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model(X_train[0].shape, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.95,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 128\n",
    "train_generator = gtzan_generator(X_train, y_train)\n",
    "steps_per_epoch = np.ceil(len(X_train)/bsize)\n",
    "\n",
    "validation_generator = gtzan_generator(X_test, y_test)\n",
    "val_steps = np.ceil(len(X_test)/bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    callbacks=[reduceLROnPlat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\r\n",
    "print(\"Val Loss = {:.3f}\".format(score[0]))\r\n",
    "print(\"Val Acc = {:.3f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist.history['accuracy'], label='train')\n",
    "plt.plot(hist.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\r\n",
    "                          normalize=False,\r\n",
    "                          title='Confusion matrix',\r\n",
    "                          cmap=plt.cm.Blues):\r\n",
    "    if normalize:\r\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
    "        print(\"Normalized confusion matrix\")\r\n",
    "    else:\r\n",
    "        print('Confusion matrix, without normalization')\r\n",
    "\r\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
    "    plt.title(title)\r\n",
    "    plt.colorbar()\r\n",
    "    tick_marks = np.arange(len(classes))\r\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
    "    plt.yticks(tick_marks, classes)\r\n",
    "\r\n",
    "    fmt = '.2f' if normalize else 'd'\r\n",
    "    thresh = cm.max() / 2.\r\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\r\n",
    "                 horizontalalignment=\"center\",\r\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
    "\r\n",
    "    plt.tight_layout()\r\n",
    "    plt.ylabel('True')\r\n",
    "    plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(X_test), axis = 1)\n",
    "y_orig = np.argmax(y_test, axis = 1)\n",
    "cm = confusion_matrix(preds, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, keys, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_count(scr):\r\n",
    "    values, counts = np.unique(scr,return_counts=True)\r\n",
    "    ind = np.argmax(counts)\r\n",
    "    return values[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_songs = np.split(np.argmax(preds, axis=1), 300)\r\n",
    "scores_songs = [vote_count(scores) for scores in scores_songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.split(np.argmax(y_test, axis=1), 300)\r\n",
    "label = [vote_count(l) for l in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "print(\"voting = {:.3f}\".format(accuracy_score(label, scores_songs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classical approach we passed from a 78.8% and in CNN we passed the accuracy to **82%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/cnn_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d04c2ccdc77879c44c0e38a475c5c944666b751d27f26244a0b0bd35fcecd56"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}